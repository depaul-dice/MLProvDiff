{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SGConv\n",
    "import pickle\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = f'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphsage architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(torch.nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    input dimension: dimension of the feature vector\n",
    "    output dimension: dimension of the node (this should be equal to the dmension of the trace)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = SAGEConv(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, adj_t = data.x, data.edge_index\n",
    "        x = self.conv1(x, adj_t)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        x = self.conv2(x, adj_t)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        x = self.conv3(x, adj_t)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        #return torch.log_softmax(x, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This will the one-hot encoded labels of all the nodes in the graph and output \n",
    "    a output_dim long vector\n",
    "\n",
    "    input dim: vocab size\n",
    "    output dim: 100 (a hyperparameter)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_fc = nn.Linear(input_dim, 200)\n",
    "        self.hidden_fc = nn.Linear(200,150)\n",
    "        self.output_fc = nn.Linear(150, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = [num_nodes,vocab_size] (412, 412)\n",
    "        h_1 = F.relu(self.input_fc(x))\n",
    "        # h_1 = [num_nodes, 200] (412, 200)\n",
    "\n",
    "        h_2 = F.relu(self.hidden_fc(h_1))\n",
    "        # h_2 = [num_nodes, 150] (412, 150)\n",
    "\n",
    "        output_mlp = self.output_fc(h_2)\n",
    "        #output_mlp = [num_nodes, output_dim] (412, 100)\n",
    "\n",
    "        return output_mlp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size):\n",
    "\t\tsuper(lstm, self).__init__()\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.output_size = output_size\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.vocab_size = vocab_size\n",
    "\t\tself.lstm = nn.LSTM(vocab_size, hidden_size)\n",
    "\t\tself.label = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "\n",
    "\tdef forward(self, input_sentence, batch_size=None):\n",
    "\t\tinput = input_sentence.permute(1, 0, 2) \n",
    "\t\th_0 = Variable(torch.zeros(1, batch_size, self.hidden_size))\n",
    "\t\tc_0 = Variable(torch.zeros(1, batch_size, self.hidden_size))\n",
    "\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
    "\t\tfinal_output = self.label(output) \n",
    "\t\treturn (final_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_mlp, optimizer_mlp, \n",
    "        model_graphsage, optimizer_graphsage,\n",
    "        model_lstm, optimizer_lstm,\n",
    "        data_nodeLabels, data_adjacencyMatrix):\n",
    "    \n",
    "\n",
    "    model_lstm.train()\n",
    "    it = iter(train_iter)\n",
    "    for i, batch in enumerate(it):\n",
    "\n",
    "        model_mlp.train()\n",
    "        optimizer_mlp.zero_grad()\n",
    "        out_mlp = model_mlp(data_nodeLabels)\n",
    "        print(\"MLP SUCCESS\")\n",
    "        \n",
    "        # create a graphsage custom data\n",
    "        data = Data(x=out_mlp, edge_index=adj_2d.t().contiguous())\n",
    "\n",
    "        #call graphsage\n",
    "        model_graphsage.train()\n",
    "        optimizer_graphsage.zero_grad()\n",
    "        out_graphsage = model_graphsage(data)\n",
    "        print(\"GRAPHSAGE SUCCESS\")\n",
    "\n",
    "    \n",
    "        output_lstm = model_lstm(batch, batch_size)\n",
    "        output_lstm = torch.reshape(output_lstm,(2, 5, 50)) #reshape it to batch_lenght, seq_length, output_dim\n",
    "\n",
    "        loss_calc = calculate_loss(output_lstm, out_graphsage, batch)\n",
    "        \n",
    "        loss_calc.backward()\n",
    "        optimizer_graphsage.step()\n",
    "        optimizer_mlp.step()\n",
    "        optimizer_lstm.step()\n",
    "    return (\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(output_lstm, out_graphsage, batch):\n",
    "    out_graphsage_reshaped = torch.transpose(out_graphsage, 0, 1)\n",
    "    c = torch.matmul(output_lstm, out_graphsage_reshaped)\n",
    "    sm = nn.Softmax(dim=2)\n",
    "    c = sm(c)\n",
    "\n",
    "  \n",
    "    loss_calc = loss_fn(c,batch)\n",
    "    \n",
    "    return (loss_calc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting sparse adjacency matrix to dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_adjacency_matrix():\n",
    "    \"\"\"\n",
    "    This method is to convert the original adjacency matrix into a 2 dimensional \n",
    "    adjacency matrix\n",
    "    \"\"\"\n",
    "    adj_2d = []\n",
    "    t_ind = 0\n",
    "    for t in data_adjacencyMatrix:\n",
    "        elem_ind = 0\n",
    "        for elem in t:\n",
    "            if elem == 1:\n",
    "                a= [t_ind, elem_ind]\n",
    "                adj_2d.append(a)\n",
    "            elem_ind += 1\n",
    "        t_ind = t_ind+1\n",
    "    return torch.tensor(adj_2d)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a demo trace data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = [[0,1,4,15,38],\n",
    "[0,1,6,18,40],\n",
    "[0,1,4,15,38],\n",
    "[0,1,6,18,40]]\n",
    "\n",
    "all_traces_ohe = []\n",
    "for t in traces:\n",
    "    trace_ohe = []\n",
    "    for t_node in t:\n",
    "        node_pos = [0]*412\n",
    "        #node_pos.append(t_node)# attaching the node label\n",
    "        node_pos[t_node] = 1\n",
    "        trace_ohe.append(node_pos)\n",
    "    all_traces_ohe.append(trace_ohe)\n",
    "\n",
    "train_data= torch.FloatTensor(all_traces_ohe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import nodes and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nodeLabels = pickle.load(open(\"/home/dhruvs/depaul_data/nodeLabels.pkl\", \"rb\"))\n",
    "data_adjacencyMatrix_old = pickle.load(open(\"/home/dhruvs/depaul_data/adjacencyMatrix.pkl\", \"rb\"))\n",
    "data_adjacencyMatrix = data_adjacencyMatrix_old.type(torch.int64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining hyperparameters and initiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP SUCCESS\n",
      "GRAPHSAGE SUCCESS\n",
      "MLP SUCCESS\n",
      "GRAPHSAGE SUCCESS\n",
      "success\n",
      "MLP SUCCESS\n",
      "GRAPHSAGE SUCCESS\n",
      "MLP SUCCESS\n",
      "GRAPHSAGE SUCCESS\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4 \n",
    "epochs = 2\n",
    "\n",
    "\n",
    "in_dim_mlp = data_nodeLabels.shape[1] #current vocab size\n",
    "output_dim_mlp = 100 # this is the output dimension of mlp (a hyperparameter)\n",
    "\n",
    "input_dim_graphsage = output_dim_mlp # input dimension of graphsage = output dimension of mlp\n",
    "hidden_dim_graphsage = 75 #this is a hyperparameter\n",
    "output_dim_graphsage = 50 #this is a hyperparameter\n",
    "\n",
    "model_mlp = MLP(input_dim=in_dim_mlp, \n",
    "                 output_dim= output_dim_mlp)\n",
    "optimizer_mlp = torch.optim.Adam(model_mlp.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "model_graphsage = GraphSAGE(in_dim=input_dim_graphsage, \n",
    "                 hidden_dim=hidden_dim_graphsage, \n",
    "                 out_dim= output_dim_graphsage)\n",
    "optimizer_graphsage = torch.optim.Adam(model_graphsage.parameters(), lr=lr)\n",
    "\n",
    "vocab_size = in_dim_mlp\n",
    "batch_size = 2 \n",
    "output_size_lstm = output_dim_graphsage\n",
    "hidden_size_lstm = 100\n",
    "train_iter = DataLoader((train_data), batch_size = batch_size)\n",
    "\n",
    "\n",
    "model_lstm = lstm(batch_size, output_size_lstm, hidden_size_lstm, vocab_size)\n",
    "optimizer_lstm = torch.optim.Adam(model_lstm.parameters(), lr=lr)\n",
    "\n",
    "adj_2d = structure_adjacency_matrix()\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, 1 + epochs):\n",
    "    success = train(model_mlp, optimizer_mlp, \n",
    "        model_graphsage, optimizer_graphsage,\n",
    "        model_lstm, optimizer_lstm,\n",
    "        data_nodeLabels, adj_2d)\n",
    "    print(success)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some notes\n",
    "\n",
    "* Currently, I am not dealing with variable length of traces\n",
    "* I am treating one hot encoded traces as ground truth (which is fine in this case), \n",
    "but we need a better way to pass the ground truth to the training module\n",
    "* Write the Evaluate function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
