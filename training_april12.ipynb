{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip3 install clean-fid numba numpy torch==2.0.0+cu118 torchvision --force-reinstall --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "#pip install pyg_lib torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-2.0.0+cu118html\n",
    "#pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cu117.html\n",
    "# check torch, torch-sparse, torch-scatter, pytorch_geometric versions\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SGConv\n",
    "import pickle\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dhruvsinha/DePaul-Reproducible-ML/running_pipeline'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphsage Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(torch.nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    input dimension: dimension of the feature vector\n",
    "    output dimension: dimension of the node (this should be equal to the dmension of the trace)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = SAGEConv(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, adj_t = data.x, data.edge_index\n",
    "        x = self.conv1(x, adj_t)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        x = self.conv2(x, adj_t)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        x = self.conv3(x, adj_t)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        #return torch.log_softmax(x, dim=-1)\n",
    "        #print(\"output graphsage\", x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This will the one-hot encoded labels of all the nodes in the graph and output \n",
    "    a output_dim long vector\n",
    "\n",
    "    input dim: vocab size\n",
    "    output dim: 100 (a hyperparameter)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_fc = nn.Linear(input_dim, 200)\n",
    "        self.hidden_fc = nn.Linear(200,150)\n",
    "        self.output_fc = nn.Linear(150, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = [num_nodes,vocab_size] (412, 412)\n",
    "        h_1 = F.relu(self.input_fc(x))\n",
    "        # h_1 = [num_nodes, 200] (412, 200)\n",
    "\n",
    "        h_2 = F.relu(self.hidden_fc(h_1))\n",
    "        # h_2 = [num_nodes, 150] (412, 150)\n",
    "\n",
    "        output_mlp = self.output_fc(h_2)\n",
    "        #output_mlp = [num_nodes, output_dim] (412, 100)\n",
    "\n",
    "        return output_mlp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class lstm(nn.Module):\n",
    "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size):\n",
    "\t\tsuper(lstm, self).__init__()\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.output_size = output_size\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.vocab_size = vocab_size\n",
    "\t\tself.lstm = nn.LSTM(vocab_size, hidden_size)\n",
    "\t\tself.label = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "\tdef forward(self, input_sentence, batch_size=None):\n",
    "\t\tinput = input_sentence.permute(1, 0, 2) \n",
    "\t\th_0 = Variable(torch.zeros(1, batch_size, self.hidden_size))\n",
    "\t\tc_0 = Variable(torch.zeros(1, batch_size, self.hidden_size))\n",
    "\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
    "\t\tfinal_output = self.label(output) \n",
    "\t\t#print(\"final_output from lstm\", final_output)\n",
    "\t\treturn (final_output)\n",
    "\"\"\"\n",
    "\n",
    "class BRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_dim):\n",
    "        super(BRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True,\n",
    "                            bidirectional = True)\n",
    "        self.fc = nn.Linear(hidden_size*2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)\n",
    "\n",
    "        out, (hidden_state, cell_state) = self.lstm(x,(h0,c0))\n",
    "        #print(\"output shape inside lstm\", out.shape)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_mlp, optimizer_mlp, \n",
    "        model_graphsage, optimizer_graphsage,\n",
    "        model_lstm, optimizer_lstm,\n",
    "        data_nodeLabels, data_adjacencyMatrix):\n",
    "    \n",
    "\n",
    "    model_lstm.train()\n",
    "    it = iter(train_iter)\n",
    "    accuracy_batch_list = []\n",
    "    loss_batch_list = []\n",
    "    for i, batch in enumerate(it):\n",
    "\n",
    "        model_mlp.train()\n",
    "        optimizer_mlp.zero_grad()\n",
    "        out_mlp = model_mlp(data_nodeLabels)\n",
    "        \n",
    "        # create a graphsage custom data\n",
    "        data = Data(x=out_mlp, edge_index=adj_2d.t().contiguous())\n",
    "\n",
    "        #call graphsage\n",
    "        model_graphsage.train()\n",
    "        optimizer_graphsage.zero_grad()\n",
    "        out_graphsage = model_graphsage(data)\n",
    "\n",
    "        model_lstm.train()\n",
    "        optimizer_lstm.zero_grad()\n",
    "        output_lstm = model_lstm(batch)\n",
    "        #print(\"shape of output from lstm\", output_lstm.shape)\n",
    "        output_lstm = torch.reshape(output_lstm,(batch_size, trace_length, output_dim_graphsage)) #reshape it to batch_lenght, seq_length, output_dim\n",
    "\n",
    "        loss_calc, result = calculate_loss(output_lstm, out_graphsage, batch)\n",
    "        accuracy = evaluate_training(result, batch)\n",
    "        accuracy_batch_list.append(accuracy)\n",
    "        loss_batch_list.append(loss_calc.item())\n",
    "        #print(\"loss\", loss_calc)\n",
    "        loss_calc.backward()\n",
    "        #print(\"grad of input fc of MLP\", model_mlp.input_fc.weight.grad)\n",
    "        optimizer_graphsage.step()\n",
    "        #print(\"grad of input fc of lstm\", list(model_lstm.parameters())) \n",
    "        optimizer_mlp.step()\n",
    "        optimizer_lstm.step()\n",
    "    accuracy_entire_epoch = sum(accuracy_batch_list)/len(accuracy_batch_list)\n",
    "    loss_entire_epoch = sum(loss_batch_list)/len(loss_batch_list)\n",
    "    \n",
    "    return (accuracy_entire_epoch, loss_entire_epoch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(output_lstm, out_graphsage, batch):\n",
    "    # batch here is the true representation. This is because the vectors in batch are \n",
    "    # one hot encoded traces\n",
    "    out_graphsage_reshaped = torch.transpose(out_graphsage, 0, 1)\n",
    "    dot_product = torch.matmul(output_lstm, out_graphsage_reshaped)\n",
    "    sm = nn.Softmax(dim=2)\n",
    "    \n",
    "    dot_product = sm(dot_product)\n",
    "    \n",
    "    loss_calc = loss_fn(dot_product,batch)\n",
    "    #evaluate_training(c,batch)\n",
    "    return (loss_calc, dot_product)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_training(result, truth):\n",
    "    \n",
    "    argmaxed_result = torch.argmax(result, dim=2)\n",
    "    argmaxed_truth = torch.argmax(truth, dim=2)\n",
    "    \n",
    "    number_correctly_mapped = 0\n",
    "    for i in range(len(argmaxed_result)):\n",
    "        trace_res =  argmaxed_result[i]\n",
    "        trace_orig = argmaxed_truth[i]\n",
    "        if torch.equal(trace_res,trace_orig):\n",
    "            number_correctly_mapped+= 1\n",
    "    \n",
    "    accuracy = number_correctly_mapped/len(argmaxed_result)\n",
    "\n",
    "    return accuracy\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting sparse adjacency matrix to dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_adjacency_matrix():\n",
    "    \"\"\"\n",
    "    This method is to convert the original adjacency matrix into a 2 dimensional \n",
    "    adjacency matrix\n",
    "    \"\"\"\n",
    "    adj_2d = []\n",
    "    t_ind = 0\n",
    "    for t in data_adjacencyMatrix:\n",
    "        elem_ind = 0\n",
    "        for elem in t:\n",
    "            if elem == 1:\n",
    "                a= [t_ind, elem_ind]\n",
    "                adj_2d.append(a)\n",
    "            elem_ind += 1\n",
    "        t_ind = t_ind+1\n",
    "    return torch.tensor(adj_2d)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import trace data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 6])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces = pickle.load(open(\"/home/dhruvsinha/DePaul-Reproducible-ML/running_pipeline/traces.pkl\", \"rb\"))\n",
    "traces = traces.int()\n",
    "#traces = traces[:9900]\n",
    "traces.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import nodes and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nodeLabels = pickle.load(open(\"/home/dhruvsinha/DePaul-Reproducible-ML/running_pipeline/nodeLabels.pkl\", \"rb\"))\n",
    "data_adjacencyMatrix_old = pickle.load(open(\"/home/dhruvsinha/DePaul-Reproducible-ML/running_pipeline/adjacencyMatrix.pkl\", \"rb\"))\n",
    "\n",
    "data_adjacencyMatrix = data_adjacencyMatrix_old.type(torch.int64)\n",
    "\n",
    "\n",
    "num_nodes_graph = data_adjacencyMatrix.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting traces to OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 6, 1365])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "traces = [[0,1,4,15,38],\n",
    "[0,1,6,18,40],\n",
    "[0,1,4,15,38],\n",
    "[0,1,6,18,40]]\n",
    "\"\"\"\n",
    "all_traces_ohe = []\n",
    "for t in traces:\n",
    "    trace_ohe = []\n",
    "    for t_node in t:\n",
    "        node_pos = [0]*num_nodes_graph\n",
    "        #node_pos.append(t_node)# attaching the node label\n",
    "        node_pos[t_node] = 1\n",
    "        trace_ohe.append(node_pos)\n",
    "    all_traces_ohe.append(trace_ohe)\n",
    "\n",
    "all_data= torch.FloatTensor(all_traces_ohe)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape torch.Size([900, 6, 1365])\n",
      "test data shape torch.Size([100, 6, 1365])\n"
     ]
    }
   ],
   "source": [
    "train_data = all_data[:len(all_data)-100]\n",
    "test_data = all_data[len(all_data)-100:]\n",
    "print(\"train data shape\", train_data.shape)\n",
    "print(\"test data shape\", test_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining hyperparameters and initiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy for epoch 1 =  0.0\n",
      "training loss for epoch 1 =  0.007875865635772547\n",
      "training accuracy for epoch 2 =  0.0\n",
      "training loss for epoch 2 =  0.007875864963150687\n",
      "training accuracy for epoch 3 =  0.0\n",
      "training loss for epoch 3 =  0.007875865118371116\n",
      "training accuracy for epoch 4 =  0.0\n",
      "training loss for epoch 4 =  0.007875864704449972\n",
      "training accuracy for epoch 5 =  0.0\n",
      "training loss for epoch 5 =  0.007875864549229542\n",
      "training accuracy for epoch 6 =  0.0\n",
      "training loss for epoch 6 =  0.00787586434226897\n",
      "training accuracy for epoch 7 =  0.0\n",
      "training loss for epoch 7 =  0.007875864187048541\n",
      "training accuracy for epoch 8 =  0.0\n",
      "training loss for epoch 8 =  0.007875863617906967\n",
      "training accuracy for epoch 9 =  0.0\n",
      "training loss for epoch 9 =  0.007875863617906967\n",
      "training accuracy for epoch 10 =  0.0\n",
      "training loss for epoch 10 =  0.007875863773127397\n",
      "training accuracy for epoch 11 =  0.0\n",
      "training loss for epoch 11 =  0.007875863410946395\n",
      "training accuracy for epoch 12 =  0.0\n",
      "training loss for epoch 12 =  0.007875863462686539\n",
      "training accuracy for epoch 13 =  0.0\n",
      "training loss for epoch 13 =  0.007875863255725967\n",
      "training accuracy for epoch 14 =  0.0\n",
      "training loss for epoch 14 =  0.007875862841804823\n",
      "training accuracy for epoch 15 =  0.0\n",
      "training loss for epoch 15 =  0.007875862945285108\n",
      "training accuracy for epoch 16 =  0.0\n",
      "training loss for epoch 16 =  0.007875862531363964\n",
      "training accuracy for epoch 17 =  0.0\n",
      "training loss for epoch 17 =  0.007875862220923105\n",
      "training accuracy for epoch 18 =  0.0\n",
      "training loss for epoch 18 =  0.007875862220923105\n",
      "training accuracy for epoch 19 =  0.0\n",
      "training loss for epoch 19 =  0.007875862065702677\n",
      "training accuracy for epoch 20 =  0.0\n",
      "training loss for epoch 20 =  0.007875861651781533\n",
      "training accuracy for epoch 21 =  0.0\n",
      "training loss for epoch 21 =  0.007875861496561103\n",
      "training accuracy for epoch 22 =  0.0\n",
      "training loss for epoch 22 =  0.007875860823939243\n",
      "training accuracy for epoch 23 =  0.0\n",
      "training loss for epoch 23 =  0.007875861651781533\n",
      "training accuracy for epoch 24 =  0.0\n",
      "training loss for epoch 24 =  0.007875860306537814\n",
      "training accuracy for epoch 25 =  0.0\n",
      "training loss for epoch 25 =  0.007875859789136384\n",
      "training accuracy for epoch 26 =  0.0\n",
      "training loss for epoch 26 =  0.007875860461758243\n",
      "training accuracy for epoch 27 =  0.0\n",
      "training loss for epoch 27 =  0.007875858806073666\n",
      "training accuracy for epoch 28 =  0.0\n",
      "training loss for epoch 28 =  0.007875859013034238\n",
      "training accuracy for epoch 29 =  0.0\n",
      "training loss for epoch 29 =  0.007875859116514524\n",
      "training accuracy for epoch 30 =  0.0\n",
      "training loss for epoch 30 =  0.007875858754333522\n",
      "training accuracy for epoch 31 =  0.0\n",
      "training loss for epoch 31 =  0.007875857771270804\n",
      "training accuracy for epoch 32 =  0.0\n",
      "training loss for epoch 32 =  0.007875857719530662\n",
      "training accuracy for epoch 33 =  0.0\n",
      "training loss for epoch 33 =  0.007875857926491234\n",
      "training accuracy for epoch 34 =  0.0\n",
      "training loss for epoch 34 =  0.007875857150389088\n",
      "training accuracy for epoch 35 =  0.0\n",
      "training loss for epoch 35 =  0.007875856477767229\n",
      "training accuracy for epoch 36 =  0.0\n",
      "training loss for epoch 36 =  0.007875857098648945\n",
      "training accuracy for epoch 37 =  0.0\n",
      "training loss for epoch 37 =  0.007875855184263654\n",
      "training accuracy for epoch 38 =  0.0\n",
      "training loss for epoch 38 =  0.007875855339484083\n",
      "training accuracy for epoch 39 =  0.0\n",
      "training loss for epoch 39 =  0.007875853632059362\n",
      "training accuracy for epoch 40 =  0.0\n",
      "training loss for epoch 40 =  0.007875852959437503\n",
      "training accuracy for epoch 41 =  0.0\n",
      "training loss for epoch 41 =  0.007875853166398075\n",
      "training accuracy for epoch 42 =  0.0\n",
      "training loss for epoch 42 =  0.00787585239029593\n",
      "training accuracy for epoch 43 =  0.0\n",
      "training loss for epoch 43 =  0.007875851976374785\n",
      "training accuracy for epoch 44 =  0.0\n",
      "training loss for epoch 44 =  0.007875851407233212\n",
      "training accuracy for epoch 45 =  0.0\n",
      "training loss for epoch 45 =  0.007875849699808491\n",
      "training accuracy for epoch 46 =  0.0\n",
      "training loss for epoch 46 =  0.007875849803288778\n",
      "training accuracy for epoch 47 =  0.0\n",
      "training loss for epoch 47 =  0.007875847733683057\n",
      "training accuracy for epoch 48 =  0.0\n",
      "training loss for epoch 48 =  0.007875846181478765\n",
      "training accuracy for epoch 49 =  0.0\n",
      "training loss for epoch 49 =  0.007875846854100624\n",
      "training accuracy for epoch 50 =  0.0\n",
      "training loss for epoch 50 =  0.007875843853172328\n",
      "Process complete\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4 \n",
    "epochs = 50\n",
    "trace_length = traces.shape[1]\n",
    "\n",
    "in_dim_mlp = data_nodeLabels.shape[1] #current vocab size\n",
    "output_dim_mlp = 100 # this is the output dimension of mlp (a hyperparameter)\n",
    "\n",
    "input_dim_graphsage = output_dim_mlp # input dimension of graphsage = output dimension of mlp\n",
    "hidden_dim_graphsage = 75 #this is a hyperparameter\n",
    "output_dim_graphsage = 50 #this is a hyperparameter\n",
    "\n",
    "model_mlp = MLP(input_dim=in_dim_mlp, \n",
    "                 output_dim= output_dim_mlp)\n",
    "optimizer_mlp = torch.optim.Adam(model_mlp.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "model_graphsage = GraphSAGE(in_dim=input_dim_graphsage, \n",
    "                 hidden_dim=hidden_dim_graphsage, \n",
    "                 out_dim= output_dim_graphsage)\n",
    "optimizer_graphsage = torch.optim.Adam(model_graphsage.parameters(), lr=lr)\n",
    "\n",
    "vocab_size = in_dim_mlp\n",
    "batch_size = 50\n",
    "output_size_lstm = output_dim_graphsage\n",
    "hidden_size_lstm = 100\n",
    "train_iter = DataLoader((train_data), batch_size = batch_size)\n",
    "\n",
    "num_layers_lstm = 2\n",
    "input_size_lstm = vocab_size\n",
    "model_lstm = BRNN(input_size_lstm, hidden_size_lstm, num_layers_lstm, output_size_lstm)\n",
    "#model_lstm = BRNN(batch_size, output_size_lstm, hidden_size_lstm, vocab_size)\n",
    "optimizer_lstm = torch.optim.Adam(model_lstm.parameters(), lr=lr)\n",
    "\n",
    "adj_2d = structure_adjacency_matrix()\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, 1 + epochs):\n",
    "    accuracy_entire_epoch, loss_entire_epoch = train(model_mlp, optimizer_mlp, \n",
    "        model_graphsage, optimizer_graphsage,\n",
    "        model_lstm, optimizer_lstm,\n",
    "        data_nodeLabels, adj_2d)\n",
    "    \n",
    "    print(\"training accuracy for epoch {} = \".format(epoch), accuracy_entire_epoch)\n",
    "    print(\"training loss for epoch {} = \".format(epoch), loss_entire_epoch)\n",
    "\n",
    "print(\"Process complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuaracy is  0.0\n"
     ]
    }
   ],
   "source": [
    "def test_model(data_to_test):\n",
    "    model_mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        out_mlp = model_mlp(data_nodeLabels)\n",
    "\n",
    "    data = Data(x=out_mlp, edge_index=adj_2d.t().contiguous())\n",
    "\n",
    "    model_graphsage.eval()\n",
    "    with torch.no_grad():\n",
    "        graphsage_output = model_graphsage(data)\n",
    "\n",
    "    model_lstm.eval()\n",
    "    with torch.no_grad():\n",
    "        lstm_test_result = model_lstm(data_to_test)\n",
    "\n",
    "    accuracy = evaluate_test(lstm_test_result, data_to_test)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_test(result, truth):\n",
    "    \n",
    "    argmaxed_result = torch.argmax(result, dim=2)\n",
    "    argmaxed_truth = torch.argmax(truth, dim=2)\n",
    "    \n",
    "    number_correctly_mapped = 0\n",
    "    for i in range(len(argmaxed_result)):\n",
    "        trace_res =  argmaxed_result[i]\n",
    "        trace_orig = argmaxed_truth[i]\n",
    "        if torch.equal(trace_res,trace_orig):\n",
    "            number_correctly_mapped+= 1\n",
    "    \n",
    "    accuracy = number_correctly_mapped/len(argmaxed_result)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# testing on train data again\n",
    "test_accuracy = test_model(train_data)\n",
    "\n",
    "print(\"Test Accuaracy is \", test_accuaracy)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
